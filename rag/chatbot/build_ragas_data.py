import json
import time
from pathlib import Path
import sys

# RAG Chatbot ê´€ë ¨ ëª¨ë“ˆ ë¡œë“œ
from bot.client.lama_cpp_client import LamaCppClient
from bot.conversation.chat_history import ChatHistory
from bot.conversation.conversation_handler import answer_with_context, refine_question
from bot.conversation.ctx_strategy import get_ctx_synthesis_strategy
from bot.memory.vector_database.chroma import Chroma
from bot.model.model_registry import get_model_settings
from helpers.log import get_logger
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevance, context_precision

logger = get_logger(__name__)

# JSON ë°ì´í„° ë¡œë“œ
file_path = Path(__file__).resolve().parent.parent
with open(f"{file_path}/supcon_books_data.json", "r", encoding="utf-8") as file:
    supcon_data = json.load(file)

positive_pairs = supcon_data["positive_pairs"]
negative_pairs = supcon_data["negative_pairs"]

# RAG ëª¨ë¸ ì´ˆê¸°í™” í•¨ìˆ˜
def load_rag_chatbot():
    root_folder = Path(__file__).resolve().parent.parent
    model_folder = root_folder / "models"
    vector_store_path = root_folder / "vector_store" / "docs_index"

    # ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    model_name = "llama-3.2"  # ì‚¬ìš©í•˜ë ¤ëŠ” ëª¨ë¸ ì´ë¦„ (ë³€ê²½ ê°€ëŠ¥)
    model_settings = get_model_settings(model_name)
    llm = LamaCppClient(model_folder=model_folder, model_settings=model_settings)

    # ì±„íŒ… ê¸°ë¡ ë° ì»¨í…ìŠ¤íŠ¸ ì „ëµ ì„¤ì •
    chat_history = ChatHistory(total_length=2)
    ctx_synthesis_strategy = get_ctx_synthesis_strategy("default", llm=llm)

    # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ
    embedding = Chroma(persist_directory=str(vector_store_path))
    index = Chroma(persist_directory=str(vector_store_path), embedding=embedding)

    return llm, chat_history, ctx_synthesis_strategy, index

# RAG ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„± í•¨ìˆ˜
def get_rag_answer(question, context, llm, chat_history, ctx_synthesis_strategy, index):
    start_time = time.time()
    
    # ì§ˆë¬¸ ë¦¬íŒŒì¸ë¨¼íŠ¸
    refined_user_input = refine_question(llm, question, chat_history=chat_history)

    # ë¬¸ì„œ ê²€ìƒ‰
    retrieved_contents, _ = index.similarity_search_with_threshold(query=refined_user_input, k=2)

    # ì‘ë‹µ ìƒì„±
    if retrieved_contents:
        streamer, _ = answer_with_context(llm, ctx_synthesis_strategy, question, chat_history, retrieved_contents)
        full_response = ""
        for token in streamer:
            full_response += llm.parse_token(token)
    else:
        full_response = "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    took = time.time() - start_time
    logger.info(f"\n--- RAG ì‘ë‹µ ìƒì„± ì™„ë£Œ: {took:.2f} ì´ˆ ---")
    return full_response

# RAGAS í‰ê°€ ë°ì´í„° ë³€í™˜
llm, chat_history, ctx_synthesis_strategy, index = load_rag_chatbot()
ragas_data = []

for anchor, positive in positive_pairs:
    generated_answer = get_rag_answer(anchor, positive, llm, chat_history, ctx_synthesis_strategy, index)
    ragas_data.append({
        "question": anchor,
        "context": positive,
        "answer": generated_answer,
        "relevance": 1.0
    })

for anchor, negative in negative_pairs:
    generated_answer = get_rag_answer(anchor, negative, llm, chat_history, ctx_synthesis_strategy, index)
    ragas_data.append({
        "question": anchor,
        "context": negative,
        "answer": generated_answer,
        "relevance": 0.0
    })

# JSON íŒŒì¼ ì €ì¥
output_path = file_path / "ragas_eval_data.json"
with open(output_path, "w", encoding="utf-8") as f:
    json.dump(ragas_data, f, indent=4, ensure_ascii=False)

print(f"âœ… RAGAS í‰ê°€ ë°ì´í„°ê°€ '{output_path}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

# ğŸ”¹ RAGAS í‰ê°€ ì‹¤í–‰
dataset = Dataset.from_list(ragas_data)
results = evaluate(dataset, metrics=[faithfulness, answer_relevance, context_precision])

print("ğŸ“Š RAGAS í‰ê°€ ê²°ê³¼:")
print(results)
